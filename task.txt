# Assessment Details

You’ll build a small question-answering service over a tiny, real document set. First ship a basic similarity search. Then make it smarter with a reranker so better evidence rises to the top. Finally, show a simple before/after comparison to prove the improvement. Keep it honest, small, and reproducible.

## Goal

Build a small Q&A service over a **tiny set of documents**.

Start with a basic similarity search, then **make it better** with a reranker. Show a simple before/after comparison.

**Submission deadline is 5 days from receipt of the technical assessment email.**

## Must-haves

- Use the following ZIP file with 20 public PDFs on industrial & machine safety. Work from this pack. The sources.json will help you with keeping citations.

- **Data:** public PDFs or web pages. Keep a sources.json with title + URL for each.
- **Chunks:** split docs into sensible pieces (roughly paragraph-size). Store in SQLite.
- **Embeddings:** free local model (e.g., all-MiniLM-L6-v2) + FAISS or Chroma for search.
- **Baseline:** cosine top-k results.
- **Reranker (choose one approach):**
    - **Hybrid** = blend vector score with a keyword score (BM25/SQLite FTS).
    - **Learned** = tiny logistic regression using a few simple features (vector score, keyword score, title match, etc.).
- **Answers:** short, **grounded in the retrieved text**, with a citation to the chunk(s). If unsure, **abstain** and say why.
- **API:** one endpoint:
    - POST /ask { q, k, mode } → returns answer | null, contexts[] (with scores + links), and reranker_used.

## Deliverables

- GitHub repo with:
    - Code for: ingest/chunk, embeddings/index, baseline search, reranker, API.
    -- sources.json and your 8-question file.
    - A short README that explains: setup, how to run, the results table, and **what you learned** (1–2 paragraphs).
- Two example curl requests that work (one easy, one tricky).

## Constraints

- **No paid APIs. CPU only.**
- Keep answers **extractive** with **citations**.
- Add a simple **threshold** to decide when to abstain.
- Make outputs **repeatable** (set a seed where it matters).

## Suggested Flow

1. Chunk → embed → baseline search.
2. Add reranker (hybrid is fastest), tune lightly.
3. Build /ask that returns a short, cited answer or abstains.
4. Run your 8 questions, print the table, jot down learnings.